{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo-hunter13\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model =  True # Whether to train the model.\n",
    "summary_freq = 1000 # Frequency at which to save training statistics.\n",
    "save_freq = 5000 # Frequency at which to save model.\n",
    "env_name = \"hunter\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.997 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 10240 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 5e-5 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 1024 # How many experiences per gradient descent update step.\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file, worker_id = 2)\n",
    "print(str(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# graphs = {}\n",
    "# for brain in env.external_brain_names:\n",
    "#     graphs[brain] =  tf.Graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "#curriculum is disabled\n",
    "# def get_progress(brain):\n",
    "#     if curriculum_file is not None:\n",
    "#         if env._curriculum.measure_type == \"progress\":\n",
    "#             return steps / max_steps\n",
    "#         elif env._curriculum.measure_type == \"reward\":\n",
    "#             return last_reward\n",
    "#         else:\n",
    "#             return None\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "models = {}\n",
    "\n",
    "for brain in env.external_brain_names:\n",
    "    with tf.variable_scope(re.sub('[^0-9a-zA-Z]+', '-', brain)):\n",
    "        models[brain] = create_agent_model(env.brains[brain], lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps)\n",
    "\n",
    "\n",
    "# is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "# use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "# use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_paths = {}\n",
    "for brain in env.external_brain_names:\n",
    "    summary_paths[brain] = './summaries/{}'.format(run_path+'_'+brain)\n",
    "    if not os.path.exists(summary_paths[brain]):\n",
    "        os.makedirs(summary_paths[brain])\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps = {}\n",
    "    last_rewards = {}\n",
    "    summary_writers = {}\n",
    "    for brain in env.external_brain_names:\n",
    "        steps[brain], last_rewards[brain] = sess.run([models[brain].global_step, models[brain].last_reward])    \n",
    "        summary_writers[brain] = tf.summary.FileWriter(summary_paths[brain])\n",
    "#         if train_model:\n",
    "#             trainers[brain].write_text(summary_writers[brain], 'Hyperparameters', hyperparameter_dict, steps)\n",
    "\n",
    "#     info = env.reset(train_mode=train_model, progress=get_progress())\n",
    "    info = env.reset(train_mode=train_model)\n",
    "    trainers = {}\n",
    "    for brain in env.external_brain_names:\n",
    "        trainers[brain] = Trainer(models[brain], sess, info[brain],\n",
    "           (env.brains[brain].action_space_type == \"continuous\"),\n",
    "            (env.brains[brain].number_observations > 0),\n",
    "             (env.brains[brain].state_space_size > 0),\n",
    "              train_model)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    while min([steps[b] for b in env.external_brain_names]) <= max_steps:\n",
    "        if env.global_done:\n",
    "#             info = env.reset(train_mode=train_model, progress=get_progress())\n",
    "            info = env.reset(train_mode=train_model)\n",
    "        # Decide and take an action\n",
    "        take_action_actions = {}\n",
    "        take_action_outputs = {}\n",
    "        for brain in env.external_brain_names:\n",
    "            take_action_actions[brain], take_action_outputs[brain] = trainers[brain].take_action(\n",
    "                info[brain], env, brain, steps[brain])\n",
    "\n",
    "        try:\n",
    "            new_info = env.step(take_action_actions)\n",
    "        except:\n",
    "            print(take_action_actions)\n",
    "            raise\n",
    "        for brain in env.external_brain_names:\n",
    "            trainers[brain].add_experiences(\n",
    "                info[brain], new_info[brain],take_action_actions[brain] , take_action_outputs[brain])\n",
    "\n",
    "        info = new_info\n",
    "        for brain in env.external_brain_names:\n",
    "            trainers[brain].process_experiences(info[brain], time_horizon, gamma, lambd)\n",
    "            if len(trainers[brain].training_buffer['actions']) > buffer_size and train_model:\n",
    "                    # Perform gradient descent with experience buffer\n",
    "                    trainers[brain].update_model(batch_size, num_epoch)\n",
    "            if steps[brain] % summary_freq == 0 and steps[brain] != 0 and train_model:\n",
    "                # Write training statistics to tensorboard.\n",
    "                trainers[brain].write_summary(summary_writers[brain], brain, steps[brain], env._curriculum.lesson_number)\n",
    "            if steps[brain] % save_freq == 0 and steps[brain] != 0 and train_model:\n",
    "                # Save Tensorflow model\n",
    "                # This does not need to be for each brain \n",
    "                save_model(sess, model_path=model_path, steps=steps[brain], saver=saver)\n",
    "            steps[brain] += 1\n",
    "            sess.run(models[brain].increment_step)\n",
    "            if len(trainers[brain].stats['cumulative_reward']) > 0:\n",
    "                mean_reward = np.mean(trainers[brain].stats['cumulative_reward'])\n",
    "                sess.run(models[brain].update_reward, feed_dict={models[brain].new_reward: mean_reward})\n",
    "                last_reward = sess.run(models[brain].last_reward)\n",
    "    for brain in env.external_brain_names:\n",
    "        # Final save Tensorflow model\n",
    "        if steps[brain] != 0 and train_model:\n",
    "            save_model(sess, model_path=model_path, steps=steps[brain], saver=saver)\n",
    "env.close()\n",
    "nodes = []\n",
    "for brain in env.external_brain_names:\n",
    "    scope = (re.sub('[^0-9a-zA-Z]+', '-', brain)) + '/'\n",
    "    nodes +=[scope + x for x in [\"action\",\"value_estimate\",\"action_probs\"]]\n",
    "    \n",
    "export_graph(model_path, env_name, target_nodes=','.join(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for brain in env.external_brain_names:\n",
    "    scope = (re.sub('[^0-9a-zA-Z]+', '-', brain)) + '/'\n",
    "    nodes +=[scope + x for x in [\"action\",\"value_estimate\",\"action_probs\"]]\n",
    "    \n",
    "export_graph(model_path, env_name, target_nodes=','.join(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
